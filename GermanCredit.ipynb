{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import io, os , sys, types\n",
    "import tabulate\n",
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pydotplus\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from helper_functions import *\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "fontsz = 12\n",
    "\n",
    "# ROC Curve and Cutoff Analysis:\n",
    "# https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/One_ROC_Curve_and_Cutoff_Analysis.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "fname_germancredit = r'dataset/German.Credit.csv'\n",
    "data_raw = pd.read_csv(fname_germancredit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REMOVE COLUMNS HERE:\n",
    "if False:\n",
    "    cols_to_remove = ['residence_since', 'num_dependents', 'existing_credits']\n",
    "    for i in cols_to_remove:\n",
    "        data_raw = data_raw.drop(i, 1) # dropping the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_target = 'class'\n",
    "cols_numeric = list(data_raw.describe().columns.values)\n",
    "cols_categoric = list(set(data_raw.columns.values) - set(cols_numeric) - set([col_target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploatory analysis\n",
    "It is important that before going overboard with algorithms, we do some exploration of the data, and understand what we see before us.<br>\n",
    "This can save us a lot of time in the long-run. Excercise: Can you think examples why exploration of the data is important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table\n",
    "pd.crosstab(data_raw['class'], data_raw['account_balance'],  margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small on purpose, but feel free to play around with it at home\n",
    "data_raw.hist(figsize=(10,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between numeric variables\n",
    "data_numeric = data_raw[cols_numeric].copy(deep=True)\n",
    "corr_mat = data_numeric.corr(method='pearson')\n",
    "cbar_ticks =np.linspace(-1,1,11)\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.xticks(fontsize=fontsz+2)\n",
    "plt.yticks(fontsize=fontsz+2)\n",
    "ax = sns.heatmap(corr_mat, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_ticks(cbar_ticks)\n",
    "cbar.set_ticklabels(cbar_ticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brksCredits = np.linspace(0,80,11) # Bins for a nice looking histogram\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(data_raw['duration'], bins=brksCredits)\n",
    "plt.title('duration', fontsize=fontsz+4)\n",
    "plt.xlabel('Loan Period [Months]', fontsize=fontsz+2)\n",
    "plt.ylabel('Count', fontsize=fontsz+2)\n",
    "plt.xticks(fontsize=fontsz)\n",
    "plt.yticks(fontsize=fontsz)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.boxplot(data_raw['duration']) \n",
    "plt.title('duration boxplot', fontsize=fontsz+4)\n",
    "plt.xlabel('Credit Month', fontsize=fontsz+2) \n",
    "plt.ylabel('duration', fontsize=fontsz+2)\n",
    "plt.xticks(fontsize=fontsz)\n",
    "plt.yticks(fontsize=fontsz)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "1. What is the purpose of preprocessing?\n",
    "2. Give 1-2 examples of common preprocessing tasks\n",
    "3. Certain features don't always have data. Some of them may be null, or simply have 'no_data' entry.<br>Discuss: What do __YOU__ think we should do in such cases? Do null, or 'no_data' entries require any special preprocessing? <br>If so, can you suggest some? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convinience, we'll replace the class-labels with 0 and 1. This makes it easier for the classification models to process<br>\n",
    "<font color='red'>__'bad'__</font> --> 0\n",
    "<font color='green'>__'good'__</font> --> 1<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw['class'].replace('bad', 0, inplace=True)\n",
    "data_raw['class'].replace('good', 1, inplace=True)\n",
    "data_raw['class'] = pd.to_numeric(data_raw['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Variables\n",
    "1. What are dummy variables?<br>\n",
    "2. Why do we use dummy variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of columns before dummy-variables:\\t\", len(data_raw.columns.values))\n",
    "for i in cols_categoric:\n",
    "    dummy_ranks = pd.get_dummies(data_raw[i], prefix=i)\n",
    "    data_raw = data_raw.join(dummy_ranks)\n",
    "    data_raw = data_raw.drop(i, 1) # dropping the original categoric column (not needed - it was replaced by dummy columns)\n",
    "    \n",
    "cols_features = list(set(data_raw.columns.values) - set([col_target])) # all feature, numeric, and categoric (now dummified)\n",
    "print (\"Number of columns after dummy-variables:\\t\", len(data_raw.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Seed\n",
    "1. What is a random seed?<br>\n",
    "2. What would be the result of using the same seed for the experiment?<br>\n",
    "3. Why is it important to use random seeds in an experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test sets\n",
    "Python has a convinient function to split the dataset into train and test<br>\n",
    "1. Look at the following example and discuss: is the below data-split optimal? Why?<br>\n",
    "2. What is a balanced dataset, and when is it important to use such datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1017\n",
    "frac_train = 0.8\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_raw[cols_features], data_raw[col_target], test_size=(1-frac_train), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_b = sum(y_train == 0)\n",
    "train_g = sum(y_train == 1)\n",
    "test_b = sum(y_test == 0)\n",
    "test_g = sum(y_test == 1)\n",
    "print (\"Class ratios between each set:\")\n",
    "print (\"Trainset\")\n",
    "print (\"\\t\\tNormal class (good):\", 100*train_g/len(y_train), \"%\\t\", \"Target class (bad):\", 100*train_b/len(y_train),\"%\")\n",
    "print (\"Testset\")\n",
    "print (\"\\t\\tNormal class (good):\", 100*test_g/len(y_test), \"%\\t\", \"Target class (bad):\", 100*test_b/len(y_test),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "More about Logistic Regression examples in python can be found here:<br>\n",
    "http://blog.yhat.com/posts/logistic-regression-python-rodeo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model coefficients\n",
    "The coefficients show what logistic regression learned to be the strongest features. The larger the abs(coefficient), the strongest the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRINT_COEFFS = False\n",
    "model_coefficients = model.coef_[0]\n",
    "# calculating p_values\n",
    "p_values =[]\n",
    "for i in range(0,len(X_train.columns.values)):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(np.array(X_train.iloc[:,i]), np.array(y_train))\n",
    "    p_values.append(p_value)\n",
    "    \n",
    "if PRINT_COEFFS:\n",
    "    print (\"prediction = \",end=\"\")\n",
    "    for i in range(0,len(cols_features)):\n",
    "        print (\"+(\"+str(round(model_coefficients[i],5))+\") *\\t<<\"+str(cols_features[i])+\">>\")\n",
    "        print (\"\\t\\t\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lgm_coeffs = pd.DataFrame(data=[list(cols_features), list(model_coefficients), list(p_values)]).transpose()\n",
    "df_lgm_coeffs.columns = ['feature', 'LGM_coeff', 'p_value']\n",
    "\n",
    "# sort by absolute coefficient value\n",
    "df_lgm_coeffs = df_lgm_coeffs.reindex(df_lgm_coeffs['LGM_coeff'].abs().sort_values(inplace=False, ascending=False).index)\n",
    "# create figure\n",
    "plt.figure(figsize=[15,5])\n",
    "horiz_line_data = np.array([0 for i in range(0, len(df_lgm_coeffs))])\n",
    "plt.plot(range(0,len(df_lgm_coeffs)), horiz_line_data, 'b--') # adding horizontal line at 0\n",
    "x=range(0,len(df_lgm_coeffs))\n",
    "plt.plot(x,df_lgm_coeffs['LGM_coeff'], 'ro')  \n",
    "plt.xticks(x, df_lgm_coeffs['feature'], rotation='vertical')\n",
    "plt.xticks(fontsize=fontsz-2)\n",
    "plt.yticks(fontsize=fontsz)\n",
    "plt.title('LGM Coefficients (sorted by absolute values)', fontsize=fontsz+4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the difference between predict and predict_prob?\n",
    "2. When would we prefer one over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by p_value value\n",
    "numeric_pvalues = df_lgm_coeffs[df_lgm_coeffs['feature'].isin(cols_numeric)].copy(deep=True)\n",
    "numeric_pvalues.sort_values(['p_value'], inplace=True)\n",
    "plt.figure(figsize=[25,5])\n",
    "horiz_line_data = np.array([0 for i in range(0, len(numeric_pvalues))])\n",
    "plt.plot(range(0,len(numeric_pvalues)), horiz_line_data, 'b--') # adding horizontal line at 0\n",
    "x=range(0,len(numeric_pvalues))\n",
    "plt.plot(x,numeric_pvalues['p_value'], 'ro')  \n",
    "plt.xticks(x, numeric_pvalues['feature'], rotation='vertical')\n",
    "plt.xticks(fontsize=fontsz+4)\n",
    "plt.yticks(fontsize=fontsz+4)\n",
    "plt.title('p-values (sorted ascending)', fontsize=fontsz+4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HINT = False\n",
    "if HINT:\n",
    "    df_lgm_coeffs = df_lgm_coeffs.reindex(df_lgm_coeffs['p_value'].abs().sort_values(inplace=False, ascending=False).index)\n",
    "    df_lgm_coeffs[df_lgm_coeffs['p_value'] < 0.1]\n",
    "\n",
    "    ### TO DO\n",
    "    # lab: use the p-value to determine the features that seem to be most relevant (use p_value <= 0.10) for all *numerical* features\n",
    "    # remove 'residence_since', 'num_dependents', 'existing_credits'\n",
    "    x = df_lgm_coeffs[df_lgm_coeffs['feature'].isin(cols_numeric)]\n",
    "    x[x['p_value'] > 0.10]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC/AUC: Reciever Operating Characteristic, Area-Under-Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_test), predicted_prob)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "print (\"Area-Under-Curve:\", round(auc,4))\n",
    "# plot_ROC() is defined in helper_functions.py\n",
    "plot_ROC(fpr,tpr, fontsz, 'Receiver operating characteristic for Logistic Regression Model') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does the ROC curve tell us?\n",
    "2. When would we want to use ROC to communicate our results?<br> Hint: try to think of cases where ROC-curve would be misleading :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About thresholds (aka \"Calculating Optimal Threshold\")\n",
    "We will compare models using \"Optimal Threshold\". The optimal threshold (OT) is the threshold we select where the confusion matrix<br>\n",
    "gives optimal results, with respect to some loss function (in our case, the loss function depends on false calssifications: false positive, and negative).<br>\n",
    "We will compare models by using the results from their optimal thresholds<br>\n",
    "<br>\n",
    "Our __decision threshold__ is prediction: for x >= prediction, we __classify__ that the sample is of the positive class (1)<br>\n",
    "for x < prediction, we classify the sample as the negative class (0)<br><br>\n",
    "For convinience of presenting the result we show it by using (1-prediction-threshold).<br>\n",
    "It helps to think of that as __the top probable__ classifications. For example, the __top__ 0.1 most probable, is anything where the probability >=0.9<br><br>\n",
    "loss matrix can be found in helper_functions.py. It helps to calculate the optimal threshold of our model<br>\n",
    "for more reading, please refer to the document __\"One_ROC_Curve_and_Cutoff_Analysis.pdf\"__ in your course reading material<br>\n",
    "we use p. #7, but the other pages are interesting too!! :)<br><br>\n",
    "### Loss Function\n",
    "The loss function helps us to select the best threshold, per problem requirements (domain knowledge)<br><br>\n",
    "$Loss$ = $C_0$ + $C_{TP}$ $\\cdot$ $P$($TP$) + $C_{TN}$ $\\cdot$ $P$($TN$) + $C_{FP}$ $\\cdot$ $P$($FP$) + $C_{FN}$ $\\cdot$ $P$($FN$) \n",
    "<br><br>\n",
    "In our example we'll use a simplified version that accounts only for for misclassifications: FN and FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_cfm = metrics.confusion_matrix(y_test, predicted) # default confusion matrix, default threshold = 0.5\n",
    "\n",
    "# Set Misclassification loss weights\n",
    "c_tn = 0 # weight of true-negative\n",
    "c_tp = 0 # weight of true-positive\n",
    "c_fn = 1 # weight of false negative\n",
    "c_fp = 5 # weight of false positive\n",
    "\n",
    "# finding the optimal values using the TRAIN-SET\n",
    "train_predicted_prob = model.predict_proba(X_train)[:,1]\n",
    "loss_matrix = calculate_loss(train_predicted_prob, y_train, c_fn, c_fp, c_tp, c_tn) \n",
    "# finding optimal threshold:\n",
    "opt_thr = list(loss_matrix[loss_matrix['loss'] == loss_matrix['loss'].min()]['prediction'])[0]\n",
    "loss = loss_matrix['loss'].min()\n",
    "predicted_prob_opt = copy.deepcopy(predicted_prob)\n",
    "predicted_prob_opt[predicted_prob_opt >  opt_thr] = 1\n",
    "predicted_prob_opt[predicted_prob_opt <= opt_thr] = 0\n",
    "opt_cfm = metrics.confusion_matrix(y_test, predicted_prob_opt)\n",
    "\n",
    "# For the first example only, we'll plot the loss function vs threshold\n",
    "plt.figure(figsize=(10,5), facecolor='white')\n",
    "plt.plot(loss_matrix['prediction'], loss_matrix['loss'], 'o-')\n",
    "plt.title('Loss function for various thresholds and confusion matrices', fontsize=fontsz+4)\n",
    "plt.xlabel('Threshold', fontsize=fontsz+4)\n",
    "plt.ylabel('Loss', fontsize=fontsz+4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This may be a little hard to remember at first. Note the ordering of the confusion matrix:\\n\")\n",
    "cfm_convention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(def_cfm,['bad', 'good'], \"Default Confusion Matrix\", 0)\n",
    "plot_confusion_matrix(opt_cfm,['bad', 'good'], \"Loss-Optimized Confusion Matrix\", 1)\n",
    "plt.show()\n",
    "print(\"Optimal threshold:\\t\",round(opt_thr,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns=['Classifier', 'AUC', 'LOSS']\n",
    "compare_models = pd.DataFrame( columns=columns)\n",
    "model_perf = pd.DataFrame( columns=columns, index=[0], data=[['Logistic Regression',auc,loss]])\n",
    "compare_models = compare_models.append(model_perf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb = BernoulliNB()\n",
    "model = gnb.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_test), predicted_prob)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "print (\"Area-Under-Curve:\", round(auc,4))\n",
    "plot_ROC(fpr,tpr, fontsz, 'Receiver operating characteristic for Naive-Bayes Model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_cfm = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "# finding the optimal values using the TRAIN-SET\n",
    "train_predicted_prob = model.predict_proba(X_train)[:, 1]\n",
    "loss_matrix = calculate_loss(train_predicted_prob, y_train, c_fn, c_fp, c_tp, c_tn) \n",
    "\n",
    "# finding optimal threshold:\n",
    "opt_thr = list(loss_matrix[loss_matrix['loss'] == loss_matrix['loss'].min()]['prediction'])[0]\n",
    "loss = loss_matrix['loss'].min()\n",
    "predicted_prob_opt = copy.deepcopy(predicted_prob)\n",
    "predicted_prob_opt[predicted_prob_opt >  opt_thr] = 1\n",
    "predicted_prob_opt[predicted_prob_opt <= opt_thr] = 0\n",
    "opt_cfm = metrics.confusion_matrix(y_test, predicted_prob_opt)\n",
    "\n",
    "plot_confusion_matrix(def_cfm,['bad', 'good'], \"Default Confusion Matrix\", 0)\n",
    "plot_confusion_matrix(opt_cfm,['bad', 'good'], \"Loss-Optimized Confusion Matrix\", 1)\n",
    "plt.show()\n",
    "print(\"Optimal threshold:\\t\",round(opt_thr,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_perf = pd.DataFrame( columns=columns, index=[0], data=[['Naive-Bayes',auc,loss]])\n",
    "compare_models = compare_models.append(model_perf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Linear Discriminant Analysis (LDA) requires all variables to be float-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##X_train_org = X_train.copy(deep=True)\n",
    "##X_test_org = X_test.copy(deep=True)\n",
    "for i in X_train.columns.values:\n",
    "    X_train[i] = X_train[i].astype(float)\n",
    "for i in X_test.columns.values:\n",
    "    X_test[i] = X_test[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1 = LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None, solver='lsqr', store_covariance=False, tol=0.0001)\n",
    "clf1.fit(X_train, y_train)\n",
    "predicted = clf1.predict(X_test)\n",
    "predicted_prob = clf1.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_test), predicted_prob)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "print (\"Area-Under-Curve:\", round(auc,4))\n",
    "\n",
    "plot_ROC(fpr,tpr, fontsz, 'Receiver operating characteristic for LDA Model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_cfm = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "# finding the optimal values using the TRAIN-SET\n",
    "train_predicted_prob = clf1.predict_proba(X_train)[:,1]\n",
    "loss_matrix = calculate_loss(train_predicted_prob, y_train, c_fn, c_fp, c_tp, c_tn) \n",
    "\n",
    "# finding optimal threshold:\n",
    "opt_thr = list(loss_matrix[loss_matrix['loss'] == loss_matrix['loss'].min()]['prediction'])[0]\n",
    "loss = loss_matrix['loss'].min()\n",
    "predicted_prob_opt = copy.deepcopy(predicted_prob)\n",
    "predicted_prob_opt[predicted_prob_opt >  opt_thr] = 1\n",
    "predicted_prob_opt[predicted_prob_opt <= opt_thr] = 0\n",
    "opt_cfm = metrics.confusion_matrix(y_test, predicted_prob_opt)\n",
    "\n",
    "plot_confusion_matrix(def_cfm,['bad', 'good'], \"Default Confusion Matrix\", 0)\n",
    "plot_confusion_matrix(opt_cfm,['bad', 'good'], \"Loss-Optimized Confusion Matrix\", 1)\n",
    "plt.show()\n",
    "print(\"Optimal threshold:\\t\",round(opt_thr,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_perf = pd.DataFrame( columns=columns, index=[0], data=[['Linear Discriminant Analysis',auc,loss]])\n",
    "compare_models = compare_models.append(model_perf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanup\n",
    "##X_train = X_train_org.copy(deep=True)\n",
    "##X_test = X_test_org.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is sensitive to the size of the dataset, and might not scale good<br>\n",
    "This is since each new sample requires additional __n__ calculations (where __n__ is the size of the input)<br>\n",
    "This means that SVM ~ O(__n__^2). There are methods to drive this number down, but as a rule of thumb, it<br>\n",
    "doesn't scale very well 'out-of-the-box'<br>\n",
    "Linear-SVM is a relatively cheap (faster) version of SVM, which is not that different from Logistic Regression<br>\n",
    "In this example, we'll run __Linear SVM__<br>\n",
    "The LSVM itself would have gone faster, but the probability calculation we are forcing is also computationally<br>\n",
    "expensive. So, after you execute this part of the program, go grab a cup of tea. You earned it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RUN_SVM = False # simply takes too much time. As an excercise, try it at home "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if RUN_SVM:\n",
    "    from sklearn import svm\n",
    "    import time\n",
    "\n",
    "    print (\"Executing SVM. This is going to take a while, so go grab a cup of tea. No, not coffee. Tea! Preferably, Earl-Grey. Have lunch.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Setting probability to False will speed things up, but we'll lose the threshold calculation\n",
    "    clf = svm.SVC(probability=True, C=1.0, cache_size=8000, kernel='linear') \n",
    "    print (clf.fit(X_train, y_train))\n",
    "    print (time.time() - start_time)\n",
    "\n",
    "    print (\"Running prediction \")\n",
    "    predicted = clf.predict(X_test)\n",
    "    predicted_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(np.array(y_test), predicted_prob)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    print (\"Area-Under-Curve:\", round(auc,4))\n",
    "\n",
    "    plot_ROC(fpr,tpr, fontsz, 'Receiver operating characteristic for SVM Model') \n",
    "    \n",
    "    def_cfm = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "    # finding the optimal values using the TRAIN-SET\n",
    "    train_predicted_prob = clf.predict_proba(X_train)[:,1]\n",
    "    loss_matrix = calculate_loss(train_predicted_prob, y_train, c_fn, c_fp, c_tp, c_tn) \n",
    "    loss = loss_matrix['loss'].min()\n",
    "    \n",
    "    # finding optimal threshold:\n",
    "    opt_thr = list(loss_matrix[loss_matrix['loss'] == loss_matrix['loss'].min()]['prediction'])[0]\n",
    "    predicted_prob_opt = copy.deepcopy(predicted_prob)\n",
    "    predicted_prob_opt[predicted_prob_opt >  opt_thr] = 1\n",
    "    predicted_prob_opt[predicted_prob_opt <= opt_thr] = 0\n",
    "    opt_cfm = metrics.confusion_matrix(y_test, predicted_prob_opt)\n",
    "\n",
    "    plot_confusion_matrix(def_cfm,['bad', 'good'], \"Default Confusion Matrix\", 0)\n",
    "    plot_confusion_matrix(opt_cfm,['bad', 'good'], \"Loss-Optimized Confusion Matrix\", 1)\n",
    "    plt.show()\n",
    "    print(\"Optimal threshold:\\t\",round(opt_thr,4))\n",
    "    model_perf = pd.DataFrame( columns=columns, index=[0], data=[['Support Vector Machine',auc,loss]])\n",
    "    compare_models = compare_models.append(model_perf, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Decision Trees is a recursive-repartitioning technique, which is used to recursively split the data in order to create nodes that are<br>\n",
    "purer. A pure node is a node that consists of only 1-class of those existing in the data.<br>\n",
    "In our context, a pure node would be composed of either all-\"bad\" or all-\"good\" classes.<br>\n",
    "The advantages of DT is that it produces rules that are easy to follow, and human-readable, in contrast to other \"black-box\" algorithms, such as Random-Forest<br>\n",
    "DTs however, are prone to overfitting, which is why we need to use some parameters to avoid such behavior.<br>\n",
    "As with __Logistic Regression__, __DT__s also require categorical features to be dummified.<br>\n",
    "1. Based on what we discussed, can you offer an intuition about why DTs tend to overfit?\n",
    "2. [Advanced] Can you offer some ways to avoid overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#md = len(cols_features)    # maximum tree depth\n",
    "md = 3                     # maximum tree depth\n",
    "mf = len(cols_features)    # maximum number of features to consider\n",
    "min_leaf = 20\n",
    "criterion = 'entropy'\n",
    "model = tree.DecisionTreeClassifier(max_depth=md, max_features=mf, criterion=criterion, \n",
    "                                    min_samples_leaf=min_leaf, random_state=seed)\n",
    "clf = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing rules to disk, as \"if-then-else\" statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write rules to file\n",
    "# REMEMBER: Rules refer ONLY to the TRAINING data!\n",
    "fname_DT_rules = r'dataset\\DT_rules_output.txt'\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = mystdout = StringIO()\n",
    "get_code(clf, cols_features) # get_code() is a function from helper_functions.py\n",
    "sys.stdout = old_stdout\n",
    "# end capture\n",
    "to_text = mystdout.getvalue()\n",
    "# write file\n",
    "text_file = open(fname_DT_rules, \"w\")\n",
    "text_file.write(to_text)\n",
    "text_file.close()\n",
    "\n",
    "# Using pyplotplus and graphviz (both must be installed on the computer for this bit to work) in order to visualize the decision tree\n",
    "PYPLOT_INST = False\n",
    "GRAPHVIZ_INST = False\n",
    "\n",
    "if (PYPLOT_INST & GRAPHVIZ_INST): # ERROR!! there's some bug with deep trees (max_depth > 5)\n",
    "    outfile= r'dataset\\tree_01.dot'\n",
    "    pngfile= r'dataset\\tree_01.png'\n",
    "    dot_data = StringIO()  \n",
    "    tree.export_graphviz(clf, out_file=outfile,  \n",
    "                         feature_names=cols_features,  \n",
    "                         class_names=['Bad', 'Good'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "\n",
    "    graph = pydotplus.graph_from_dot_file(outfile)\n",
    "    graph.write_png(pngfile)\n",
    "    \n",
    "else: \n",
    "    print (\"Skipping on visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_test), predicted_prob)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "print (\"Area-Under-Curve:\", round(auc,4))\n",
    "plot_ROC(fpr,tpr, fontsz, \"Receiver operating characteristic for Decision Tree Model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_cfm = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "# finding the optimal values using the TRAIN-SET\n",
    "train_predicted_prob = clf.predict_proba(X_train)[:,1]\n",
    "loss_matrix = calculate_loss(train_predicted_prob, y_train, c_fn, c_fp, c_tp, c_tn) \n",
    "\n",
    "# finding optimal threshold:\n",
    "opt_thr = list(loss_matrix[loss_matrix['loss'] == loss_matrix['loss'].min()]['prediction'])[0]\n",
    "loss = loss_matrix['loss'].min()\n",
    "predicted_prob_opt = copy.deepcopy(predicted_prob)\n",
    "predicted_prob_opt[predicted_prob_opt >  opt_thr] = 1\n",
    "predicted_prob_opt[predicted_prob_opt <= opt_thr] = 0\n",
    "opt_cfm = metrics.confusion_matrix(y_test, predicted_prob_opt)\n",
    "\n",
    "plot_confusion_matrix(def_cfm,['bad', 'good'], \"Default Confusion Matrix\", 0)\n",
    "plot_confusion_matrix(opt_cfm,['bad', 'good'], \"Loss-Optimized Confusion Matrix\", 1)\n",
    "plt.show()\n",
    "print(\"Optimal threshold:\\t\",round(opt_thr,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_perf = pd.DataFrame( columns=columns, index=[0], data=[['Decision Tree',auc,loss]])\n",
    "compare_models = compare_models.append(model_perf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random Forest is an ensemble learning classification method, which utilizes multiple decision-trees,<br>\n",
    "and a voting mechanism in order to classify each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=md, max_features=mf, criterion=criterion, min_samples_leaf = min_leaf, random_state=seed)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "predicted_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(np.array(y_test), predicted_prob)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "print (\"Area-Under-Curve:\", round(auc,4))\n",
    "\n",
    "plot_ROC(fpr,tpr, fontsz, \"Receiver operating characteristic for Random Forest Model\") # function is defined in helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_cfm = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "# finding the optimal values using the TRAIN-SET\n",
    "train_predicted_prob = clf.predict_proba(X_train)[:,1]\n",
    "loss_matrix = calculate_loss(train_predicted_prob, y_train, c_fn, c_fp, c_tp, c_tn) \n",
    "loss = loss_matrix['loss'].min()\n",
    "\n",
    "# finding optimal threshold:\n",
    "opt_thr = list(loss_matrix[loss_matrix['loss'] == loss_matrix['loss'].min()]['prediction'])[0]\n",
    "loss = loss_matrix['loss'].min()\n",
    "predicted_prob_opt = copy.deepcopy(predicted_prob)\n",
    "predicted_prob_opt[predicted_prob_opt >  opt_thr] = 1\n",
    "predicted_prob_opt[predicted_prob_opt <= opt_thr] = 0\n",
    "opt_cfm = metrics.confusion_matrix(y_test, predicted_prob_opt)\n",
    "\n",
    "plot_confusion_matrix(def_cfm,['bad', 'good'], \"Default Confusion Matrix\", 0)\n",
    "plot_confusion_matrix(opt_cfm,['bad', 'good'], \"Loss-Optimized Confusion Matrix\", 1)\n",
    "plt.show()\n",
    "print(\"Optimal threshold:\\t\",round(opt_thr,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_perf = pd.DataFrame( columns=columns, index=[0], data=[['Random Forest',auc,loss]])\n",
    "compare_models = compare_models.append(model_perf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tabulate.tabulate(compare_models, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#thr = opt_thr\n",
    "#pptest = copy.deepcopy(predicted_prob)\n",
    "#pptest[pptest >  thr] = 1\n",
    "#pptest[pptest <= thr] = 0\n",
    "\n",
    "#print (sum((y_test ==0) & (pptest == 0) ))  # TN\n",
    "#print (sum((y_test ==0) & (pptest == 1) ))  # FP\n",
    "#print (sum((y_test ==1) & (pptest == 0) ))  # FN\n",
    "#print (sum((y_test ==1) & (pptest == 1) ))  # TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Marie Curie, Erwin Schrodinger, Enrico Fermi, Robert Oppenheimer, Albert Einstein\n",
    "# [18671107, 18870812, 19010929, 19040422, 18790314]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
